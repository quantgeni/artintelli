{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML18-entropy.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOsav1effL1q",
        "colab_type": "code",
        "outputId": "9187734f-2eca-430e-f606-4b2cbb7c7280",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "## 엔트로피 entropy\n",
        "\n",
        "# 물리 열역학의 관점에서의 정의\n",
        "# => 물질의 열적 상태를 나타내는 물리량의 하나\n",
        "# => 보통 무질서도 라고 함\n",
        "# => 엔트로피가 높으면 무질서도 증가\n",
        "\n",
        "# 통계적 관점에서의 정의\n",
        "# => 정보이득(information gain)의 혼잡도\n",
        "# => 엔트로피가 높으면 정보이해가 어려워짐\n",
        "\n",
        "# 학습데이터는 기본적으로 혼잡한 상태임\n",
        "# 따라서, 어떤 조건으로 분류해야만 전체 혼잡도가\n",
        "# 개선되는지 계산하고 그 조건을 찾아내서 실제로 정리함\n",
        "# 정리한 결과에 대해 다시 계산해서 처리를 반복 적용함\n",
        "\n",
        "# 정보이론information theory에서 정보의 불확실성을\n",
        "# 수치로 나타낸 것을 엔트로피entropy라 함\n",
        "\n",
        "\n",
        "# ex) x를 기준으로 y를 나누는 가장 좋은 방법은?\n",
        "# x = [1, 2, 3, 4, 5, 6, 7, 8]\n",
        "# y = [0, 0, 0, 1, 1, 1, 1, 1]\n",
        "\n",
        "\n",
        "# 방법 1) 분류 기준 : x < 3.5  <= 최적의 조건\n",
        "# 방법 2) 분류 기준 : x < 4.5  <= 오분류 조건\n",
        "\n",
        "\n",
        "# 엔트로피의 식\n",
        "# 복수의 현상(1~n)이 존재할 때 그 혼잡도를 나타내는 엔트로피 식은 \n",
        "# -p(현상1) * log(p(현상1)) + \n",
        "# -p(현상2) * log(p(현상2)) +\n",
        "# ... ...\n",
        "# -p(현상n) * log(p(현상n)) 이다\n",
        "\n",
        "# 참고 - 엔트로피는 일반적으로 자연로그를 이용해서 계산함!!\n",
        "# 단!! 아래 예제에서는 2를 밑으로 하는 로그로 엔트로피를 계산했음\n",
        "\n",
        "\n",
        "\n",
        "# ex) 확률 0.1의 현상이 10개 일어난 경우 엔트로피는?\n",
        "# (0.1 * log2(0.1)) * 10 * -1 = 3.3219280948873626\n",
        "\n",
        "import math\n",
        "(0.1 * math.log2(0.1)) * 10 * -1\n",
        "\n",
        "\n",
        "# ex) 확률 0.25의 현상이 4개 일어난 경우 엔트로피는?\n",
        "(0.25 * math.log2(0.25)) * 4 * -1\n",
        "\n",
        "\n",
        "\n",
        "# ex) 확률 0.5의 현상이 2개 일어난 경우 엔트로피는?\n",
        "(0.5 * math.log2(0.5)) * 2 * -1\n",
        "\n",
        "\n",
        "\n",
        "# ex) 10마리의 동물이 개인지 고양이인지 분류\n",
        "# big   follow   walking   target\n",
        "# yes   yes      yes       dog\n",
        "# yes   yes      no        cat\n",
        "# no    yes      yes       dog\n",
        "# yes   yes      yes       dog\n",
        "# no    no       yes       cat\n",
        "\n",
        "# yes   no       yes       dog\n",
        "# yes   no       yes       cat\n",
        "# no    no       no        cat\n",
        "# yes   yes      yes       dog\n",
        "# no    yes      no        dog\n",
        "\n",
        "\n",
        "# 1) target 엔트로피 계산\n",
        "# 10마리 중 6마리는 개, 4마리는 고양이로 분류\n",
        "# dog : cat = 6 : 4\n",
        "# 개로 분류될 확률       : 6/10 = 0.6\n",
        "# 고양이로 분류될 확률   : 4/10 = 0.4\n",
        "\n",
        "# 따라서, 엔트로피\n",
        "-(0.6 * log2(0.6)) -(0.4 * log2(0.4))\n",
        "=> -(0.6 * math.log2(0.6)) -(0.4 * math.log2(0.4))\n",
        "=> -0.44217935649972373 -0.5287712379549449\n",
        "=> 0.9709505944546686 = 0.971\n",
        "\n",
        "\n",
        "\n",
        "# 2) 덩치기준으로 분류시 엔트로피 계산\n",
        "\n",
        "# 덩치가 큰 것으로 분류  \n",
        "# 개 : 고양이 = 4 : 2\n",
        "# 덩치가 큰 경우 개로 분류될 확률 : 4/6 = 0.6666666666666666\n",
        "# 덩치가 큰 경우 고양이로 분류될 확률 : 2/6 = 0.3333333333333333\n",
        "# 따라서, -(0.667 * log2(0.667)) -(0.333 * log2(0.333))\n",
        "=> -(0.667 * math.log2(0.667)) -(0.333 * math.log2(0.333))\n",
        "=> 0.9179621399872384 = 0.918\n",
        "\n",
        "\n",
        "# 덩치가 작은것으로 분류\n",
        "# 개 : 고양이 = 2 : 2\n",
        "# 덩치가 작은 경우 개/고양이로 분류될 확률 2/4 = 0.5\n",
        "=> -(0.5 * math.log2(0.5)) -(0.5 * math.log2(0.5))\n",
        "=> 0.5 + 0.5 = 1.0\n",
        "\n",
        "\n",
        "# 덩치로 분류했을때의 엔트로피는\n",
        "# 개로 분류될 확률 x 덩치로 분류될 확률 과 \n",
        "# 고양이로 분류될 확률 x 덩치로 분류될 확률을\n",
        "# 더해준 결과값이 됨\n",
        "0.6 * 0.918 + 0.4 * 1 = 0.9508\n",
        "\n",
        "\n",
        "\n",
        "# 3) 따름기준으로 분류시 엔트로피 계산\n",
        "\n",
        "# 잘 따름으로 분류  \n",
        "# 개 : 고양이 = 5:1\n",
        "\n",
        "# 잘따르는데 개로 분류될 확률\n",
        "# 5/6 = 0.8333333333333334\n",
        "\n",
        "# 잘 따르는데 고양이로 분류될 확률\n",
        "# 1/6 = 0.16666666666666666\n",
        "# 따라서, \n",
        "=> -(0.833 * math.log2(0.833)) -(0.167 * math.log2(0.167))\n",
        "=> 0.21958846221401737 -0.43120735869540183\n",
        "=> 0.220 + 0.431 = 0.651 \n",
        "\n",
        "\n",
        "\n",
        "# 잘 안따름으로 분류\n",
        "# 개 : 고양이 = 1:3\n",
        "\n",
        "# 잘 안따르는데 개로 분류될 확률 \n",
        "# 1/4 = 0.25 \n",
        "\n",
        "# 잘 안따르는데 고양이로 분류되 확률\n",
        "# 3/4 = 0.75\n",
        "# 따라서, \n",
        "=> -(0.25 * math.log2(0.25)) -(0.75 * math.log2(0.75))\n",
        "=> 0.5- 0.31127812445913283 = -0.811\n",
        "\n",
        "\n",
        "\n",
        "# 덩치로 분류했을때의 엔트로피는\n",
        "# (개로 분류될 확률 * 따름으로 분류될 확률) + \n",
        "# (고양이로 분류될 확률 * 따름으로 분류될 확률)\n",
        "\n",
        "0.6 * 0.651 + 0.4 * 0.811 = 0.715\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 4) 산책기준으로 분류시 엔트로피 계산\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6730116670092565"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kU1x5IZJR5FM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 의사결정트리\n",
        "\n",
        "# 의사결정트리 결정영역을 표시하기 위해\n",
        "# mlxtend 패키지 설치\n",
        "# pip install mlxtend\n",
        "from mlxtend.plotting import plot_decision_regions\n",
        "\n",
        "\n",
        "# 데이터 불러오기\n",
        "catdog = pd.read_csv('c:/Java/data/catdog.txt')\n",
        "\n",
        "# 탐색적 분석\n",
        "catdog.info()\n",
        "sns.countplot(catdog.target)\n",
        "# sns.countplot(catdog['target'])\n",
        "plt.show()\n",
        "\n",
        "# 데이터 분석\n",
        "data = catdog.iloc[:, 0:3]\n",
        "target = catdog['target']\n",
        "\n",
        "# 의사결정트리 분석\n",
        "dtc = DecisionTreeClassifier(criterion='entropy') # , max_depth=4)\n",
        "# criterion : 정보이득 알고리즘 지정\n",
        "# max_depth : 의사결정 트리의 가지수 조정\n",
        "dtc.fit(data, target)\n",
        "print('훈련정확도', dtc.score(data, target))\n",
        "\n",
        "# 의사결정나무 시각화\n",
        "dot_data = tree.export_graphviz(dtc, out_file=None, feature_names=['big', 'follow', 'walking'],\n",
        "                                class_names=['cat', 'dog'], filled=True, rounded=True)\n",
        "graph = pydotplus.graph_from_dot_data(dot_data)\n",
        "graph.write_pdf('catdog.pdf')\n",
        "graph.write_png('catdog.png')\n",
        "\n",
        "img = pltimg.imread('catdog.png')\n",
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# 날씨 상황에 따라 골프 실행 여부 분석 (playgolf.txt)\n",
        "\n",
        "# 데이터 불러오기\n",
        "playgolf = pd.read_csv('c:/Java/data/playgolf.txt')\n",
        "\n",
        "playgolf.info()\n",
        "sns.countplot(playgolf.playGolf)\n",
        "plt.show()\n",
        "\n",
        "# 데이터 전처리\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "encoder = LabelEncoder()\n",
        "playgolf['temp2'] = encoder.fit_transform(playgolf['temp'])\n",
        "playgolf['outlook2'] = encoder.fit_transform(playgolf['outlook'])\n",
        "playgolf['humidity2'] = encoder.fit_transform(playgolf['humidity'])\n",
        "playgolf['windy2'] = encoder.fit_transform(playgolf['windy'])\n",
        "playgolf['playGolf2'] = encoder.fit_transform(playgolf['playGolf'])\n",
        "print(playgolf)\n",
        "\n",
        "# 데이터 분리\n",
        "data = playgolf.iloc[:, 6:10]\n",
        "target = playgolf['playGolf2']\n",
        "\n",
        "# 의사결정트리 분석\n",
        "dtc = DecisionTreeClassifier(criterion='entropy', max_depth=3)\n",
        "# criterion : 정보이득 알고리즘 지정\n",
        "# max_depth : 의사결정 트리의 가지수 조정\n",
        "dtc.fit(data, target)\n",
        "print('훈련정확도', dtc.score(data, target))\n",
        "\n",
        "# 의사결정나무 시각화\n",
        "dot_data = tree.export_graphviz(dtc, out_file=None,\n",
        "                                feature_names=['temp2', 'outlook2', 'humidity2', 'windy2'],\n",
        "                                class_names=['no','yes'], filled=True, rounded=True)\n",
        "graph = pydotplus.graph_from_dot_data(dot_data)\n",
        "graph.write_pdf('playgolf.pdf')\n",
        "graph.write_png('playgolf.png')\n",
        "\n",
        "img = pltimg.imread('playgolf.png')\n",
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# 1) target 엔트로피 계산\n",
        "# 14개의 날씨 데이터 중 골프침yes/안침no의 횟수는 9/5임\n",
        "# 골프칠 확률   : 9/14 = 0.6428571428571429\n",
        "# 골프안칠 확률 : 5/14 = 0.35714285714285715\n",
        "# 엔트로피는 : -(9/14 *log2(9/14)) -(5/14 * log2(5/14))\n",
        "# -(0.64*np.log2(0.64)) -(0.36*np.log2(0.36)) = 0.94\n",
        "entropy([9,5], base=2)\n",
        "\n",
        "# 2) temp 기준으로 분류시 엔트로피 계산\n",
        "# hot 로 분류 :  골프침:안침 = 2:2\n",
        "# hot 골프칠 확률 : 2/4 = 0.5\n",
        "# hot 골프안칠 확률 : 2/4 = 0.5\n",
        "# 엔트로피 : -(0.5*math.log2(0.5)) *2 = 1\n",
        "entropy([2,2], base=2)\n",
        "\n",
        "# cool 분류 :  골프침:안침 = 3:1\n",
        "# cool 골프칠 확률 : 3/4 = 0.75\n",
        "# cool 골프안칠 확률 : 1/4 = 0.25\n",
        "# 엔트로피 : -(0.75*math.log2(0.75)) -(0.25*math.log2(0.25)) = 0.8112781244591328 = 0.81\n",
        "entropy([3,1], base=2)\n",
        "\n",
        "# mild 분류 :  골프침:안침 = 4:2\n",
        "# mild 골프칠 확률 : 4/6 = 0.67\n",
        "# mild 골프안칠 확률 : 2/6 = 0.33\n",
        "# 엔트로피 : -(0.67*math.log2(0.67)) -(0.33*math.log2(0.33)) = 0.9182958340544894 = 0.92\n",
        "entropy([4,2], base=2)\n",
        "\n",
        "# temp 분류했을 때의 엔트로피는\n",
        "# (골프칠 확률 * temp로 분류될 확률) + (골프안칠 확률 * temp로 분류될 확률)\n",
        "# (0.64*0.81)+(0.36*0.97)\n",
        "# (4/14)*1 + (4/14)*0.81 + (6/14)*0.91 = 0.9071428571428571 = 0.91\n",
        "\n",
        "\n",
        "# 3) outlook\n",
        "# sunny로 분류 :  골프침:안침 = 2:3\n",
        "# sunny일때 골프칠 확률 : 2/5 = 0.4\n",
        "# sunny일때 골프안칠 확률 : 3/5 = 0.6\n",
        "# 엔트로피 : -(0.4*math.log2(0.4)) -(0.6*math.log2(0.6)) = 0.9709505944546686 = 0.97\n",
        "entropy([2,3], base=2)\n",
        "\n",
        "# overcast로 분류 :  골프침:안침 = 4:0\n",
        "# overcast일때 골프칠 확률 : 4/4 = 1\n",
        "# overcast일때 골프안칠 확률 : 0/5 = 0\n",
        "# 엔트로피 : -(1*math.log2(1)) -(0*math.log2(0)) = 0 (분류불가)\n",
        "entropy([4,0], base=2)\n",
        "\n",
        "# rain로 분류 :  골프침:안침 = 3:2\n",
        "# rain일때 골프칠 확률 : 3/5 = 0.6\n",
        "# rain일때 골프안칠 확률 : 2/5 = 0.4\n",
        "# 엔트로피 : -(0.6*math.log2(0.6)) -(0.4*math.log2(0.4)) = 0.9709505944546686 = 0.97\n",
        "entropy([3,2], base=2)\n",
        "\n",
        "# outlook으로 분류했을 때의 엔트로피는\n",
        "# high/overcast/rain 일때의 확률에 high/overcast/rain 일때\n",
        "# 골프 칠/안칠 확률(엔트로피)을 곱해서 더해준 값\n",
        "# (5/14)*0.97 + (4/14)*0 + (5/14)*0.97 = 0.69\n",
        "\n",
        "\n",
        "# 4) humidity\n",
        "# high 로 분류 : 골프침:안침 = 3:4\n",
        "# 습도가 높을때 골프칠 확률 : 3/7 = 0.42857142857142855\n",
        "# 습도가 높을때 골프안칠 확률 : 4/7 = 0.5714285714285714\n",
        "# 엔트로피 : -(0.43*math.log2(0.43)) -(0.57*math.log2(0.57)) = 0.9858150371789198 = 0.98\n",
        "entropy([3,4], base=2)\n",
        "\n",
        "# normal      : 골프침:안침 = 6:1\n",
        "# 습도가 보통일때 골프칠 확률 : 6/7 = 0.8571428571428571\n",
        "# 습도가 보통일때 골프안칠 확률 : 1/7 = 0.14285714285714285\n",
        "# 엔트로피 : -(0.86*math.log2(0.86)) -(0.14*math.log2(0.14)) = 0.584238811642856 = 0.58\n",
        "entropy([6,1], base=2)\n",
        "\n",
        "# 습도로 분류했을 때의 엔트로피는\n",
        "# high/normal일때의 확률에 high/normal일때 골프 칠/안칠 확률(엔트로피)을 곱해서 더해준 값\n",
        "# ((7/14)*0.58)+((7/14)*0.98) = 0.78\n",
        "\n",
        "\n",
        "# 5) windy\n",
        "# True 로 분류 : 골프침:안침 = 3:3\n",
        "entropy([3,3], base=2) # 1\n",
        "\n",
        "# False 로 분류 : 골프침:안침 = 6:2\n",
        "# windy가 높을때 골프칠 확률 : 3/7 = 0.42857142857142855\n",
        "# windy가 높을때 골프안칠 확률 : 4/7 = 0.5714285714285714\n",
        "entropy([6,2], base=2) # 0.8112781244591328\n",
        "# ((6/14)*1)+((8/14)*0.81) = 0.8914285714285715\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}