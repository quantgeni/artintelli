{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[NLP. Tensorflow+Keras15] Text Mining4 (KoNLPy. 한국어전용NLP).ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"x2gLm3D9agx8","colab_type":"text"},"source":["# KoNLPy를 통한 한국어 자연어처리 (Korean Natural Language Processing [NLP]) by  유형균"]},{"cell_type":"code","metadata":{"id":"SbmuA9c8uS4q","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"a93069ea-93c0-4b28-e4f2-96405e28fa4c","executionInfo":{"status":"ok","timestamp":1563785416207,"user_tz":-540,"elapsed":24900,"user":{"displayName":"얍얍","photoUrl":"","userId":"00095991694523741423"}}},"source":["##### ------------------ 본 한글주석 by 유형균 (이메일 문의 : quantgeni@gmail.com) ------------------ #####\n","# 다양한 색상 팔레트를 이용한 워드클라우드\n","# jiffyclub.github.io/palettable\n","\n","# 여러가지 배색들을 도와주는 패키지\n","\n","# 말뭉치 fam\n","from konlpy.corpus import kolaw\n","from konlpy.corpus import kobill\n","\n","\n","!pip install palettable\n","!pip install colorbrewer\n","\n","from wordcloud import ImageColorGenerator\n","\n","\n","import random\n","from palettable.colorbrewer.sequential import Reds_9\n","from palettable.colorbrewer.diverging import RdYlBu_11\n","from palettable.colorbrewer.qualitative import Pastel2_8\n","\n","import numpy as np\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","\n","import nltk\n","nltk.download('book', quiet=True)\n","from nltk.book import * # text1 부터 text9까지 다양한 텍스트가 제공\n","\n","nltk.corpus.gutenberg.fileids() #저작권이 말소된 문학작품을 포함하는 구텐베르그 말뭉치를 확인\n","nltk.corpus.gutenberg.raw('shakespeare-hamlet.txt')\n","nltk.download('wordnet')\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","\n","from nltk.tag import pos_tag\n","from nltk import sent_tokenize\n","from nltk import word_tokenize\n","\n","# 원형 찾아주기\n","from nltk.stem import LancasterStemmer\n","from nltk.stem import WordNetLemmatizer # 위 stemmer보단, 아래 lemmatizer가 성능이 좋음. 단 후자의 경우 품사 지정해줘야!\n","\n","from nltk.tag import pos_tag\n","from nltk import FreqDist\n","from wordcloud import WordCloud\n","import matplotlib.pyplot as plt\n","from collections import Counter\n","\n","# KoNLPy\n","# NLTK는 영어 정보처리를 위한 패키지\n","# 반면, KoNLPy는 한국어 정보처리를 위한 패키지임\n","\n","# 서울대학교 산업공학과가 개발한 형탯호 분석기\n","# 래퍼 피키지인데, 이미 개발된 한글 형태소 분석기를\n","# 파이썬에서 바로 사용할 수 있도록 도와줌\n","\n","# 현재까지 지원하는 형태소 분석기는 모두 5가지\n","# => Hannanum, Kkma, Komoran, Mecab , Okt (구 Twitter)\n","\n","# konlpy-ko.readthedocs.io \n","# konlpy.org\n","\n","# KoNLPy 설치 전 필수 패키지가 JPype1인데\n","# Visual C++ 14 버전 필요!! => visualstudio\n","# ==> 하지만, 미리 컴파일된 ㅍ ㅐ키지가 배포됨\n","# www.lfd.uci.edu/~gohlke/pythonlibs\n","# JPype1 패키지를 다운로드한 후 설치함\n","# JPype1‑0.7.0‑cp36‑cp36m‑win_amd64.whl\n","\n","# !pip install JPype1‑0.7.0‑cp36‑cp36m‑win_amd64.whl\n","\n","# colab에서는 위 JPype1 별도설치 안해도 됨\n","\n","!pip install konlpy\n","import konlpy\n","from konlpy.tag import Hannanum\n","from konlpy.tag import Kkma\n","from konlpy.tag import Komoran\n","from konlpy.tag import Mecab\n","from konlpy.tag import Okt # 구 Twitter\n","\n","##### ------------------ 본 한글주석 by 유형균 (이메일 문의 : quantgeni@gmail.com) ------------------ #####\n","# 다양한 색상 팔레트를 이용한 워드클라우드\n","# jiffyclub.github.io/palettable\n","\n","# 여러가지 배색들을 도와주는 패키지\n","\n","!pip install palettable\n","!pip install colorbrewer\n","\n","from wordcloud import ImageColorGenerator\n","\n","\n","import numpy as np\n","from PIL import Image\n","# KoNLPy\n","\n","# NLTK 는 영어 정보처리를 위한 패키지\n","# 반면, KoNLPy 는 한국어 정보처리를 위한 패키지임\n","\n","# 서울대학교 산업공학과에서 개발한 형태소 분석기 래퍼 패키지인데,\n","# 이미 개발된 한글 형태소 분석기를 파이썬에서 바로 사용할 수 있도록 도와줌\n","\n","# 현재까지 지원하는 형태소 분석기는 모두 5가지\n","# => Hannanum, Kkma, Komoran, mecab, Okt\n","\n","# konlpy-ko.readthedocs.io / konlpy.org\n","\n","# konlpy 설치전 필수 패키지가 JPype1 인데\n","# Visual C++ 14 버젼 필요!! => visualstudio\n","# ==> 하지만, 미리 컴파일된 패키지가 배포됨\n","\n","# www.lfd.uci.edu/~gohlke/pythonlibs 에서\n","# JPype1 검색후 패키지를 다운로드한 후 설치함\n","\n","# pip install JPype1-0.7.0-cp36-cp36m-win_amd64.whl => 콜랩아닌 프로그램에서 설치\n","# pip install konlpy import matplotlib.pyplot as plt"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: palettable in /usr/local/lib/python3.6/dist-packages (3.2.0)\n","Requirement already satisfied: colorbrewer in /usr/local/lib/python3.6/dist-packages (0.2.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from colorbrewer) (1.12.0)\n","[nltk_data] Downloading collection 'book'\n","[nltk_data]    | \n","[nltk_data]    | Downloading package abc to /root/nltk_data...\n","[nltk_data]    |   Package abc is already up-to-date!\n","[nltk_data]    | Downloading package brown to /root/nltk_data...\n","[nltk_data]    |   Package brown is already up-to-date!\n","[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n","[nltk_data]    |   Package chat80 is already up-to-date!\n","[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n","[nltk_data]    |   Package cmudict is already up-to-date!\n","[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n","[nltk_data]    |   Package conll2000 is already up-to-date!\n","[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n","[nltk_data]    |   Package conll2002 is already up-to-date!\n","[nltk_data]    | Downloading package dependency_treebank to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package dependency_treebank is already up-to-date!\n","[nltk_data]    | Downloading package genesis to /root/nltk_data...\n","[nltk_data]    |   Package genesis is already up-to-date!\n","[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n","[nltk_data]    |   Package gutenberg is already up-to-date!\n","[nltk_data]    | Downloading package ieer to /root/nltk_data...\n","[nltk_data]    |   Package ieer is already up-to-date!\n","[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n","[nltk_data]    |   Package inaugural is already up-to-date!\n","[nltk_data]    | Downloading package movie_reviews to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package movie_reviews is already up-to-date!\n","[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n","[nltk_data]    |   Package nps_chat is already up-to-date!\n","[nltk_data]    | Downloading package names to /root/nltk_data...\n","[nltk_data]    |   Package names is already up-to-date!\n","[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n","[nltk_data]    |   Package ppattach is already up-to-date!\n","[nltk_data]    | Downloading package reuters to /root/nltk_data...\n","[nltk_data]    |   Package reuters is already up-to-date!\n","[nltk_data]    | Downloading package senseval to /root/nltk_data...\n","[nltk_data]    |   Package senseval is already up-to-date!\n","[nltk_data]    | Downloading package state_union to /root/nltk_data...\n","[nltk_data]    |   Package state_union is already up-to-date!\n","[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n","[nltk_data]    |   Package stopwords is already up-to-date!\n","[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n","[nltk_data]    |   Package swadesh is already up-to-date!\n","[nltk_data]    | Downloading package timit to /root/nltk_data...\n","[nltk_data]    |   Package timit is already up-to-date!\n","[nltk_data]    | Downloading package treebank to /root/nltk_data...\n","[nltk_data]    |   Package treebank is already up-to-date!\n","[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n","[nltk_data]    |   Package toolbox is already up-to-date!\n","[nltk_data]    | Downloading package udhr to /root/nltk_data...\n","[nltk_data]    |   Package udhr is already up-to-date!\n","[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n","[nltk_data]    |   Package udhr2 is already up-to-date!\n","[nltk_data]    | Downloading package unicode_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package unicode_samples is already up-to-date!\n","[nltk_data]    | Downloading package webtext to /root/nltk_data...\n","[nltk_data]    |   Package webtext is already up-to-date!\n","[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n","[nltk_data]    |   Package wordnet is already up-to-date!\n","[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n","[nltk_data]    |   Package wordnet_ic is already up-to-date!\n","[nltk_data]    | Downloading package words to /root/nltk_data...\n","[nltk_data]    |   Package words is already up-to-date!\n","[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n","[nltk_data]    |       to-date!\n","[nltk_data]    | Downloading package maxent_ne_chunker to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n","[nltk_data]    | Downloading package universal_tagset to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package universal_tagset is already up-to-date!\n","[nltk_data]    | Downloading package punkt to /root/nltk_data...\n","[nltk_data]    |   Package punkt is already up-to-date!\n","[nltk_data]    | Downloading package book_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package book_grammars is already up-to-date!\n","[nltk_data]    | Downloading package city_database to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package city_database is already up-to-date!\n","[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n","[nltk_data]    |   Package tagsets is already up-to-date!\n","[nltk_data]    | Downloading package panlex_swadesh to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n","[nltk_data]    | Downloading package averaged_perceptron_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n","[nltk_data]    |       to-date!\n","[nltk_data]    | \n","[nltk_data]  Done downloading collection book\n","Requirement already satisfied: konlpy in /usr/local/lib/python3.6/dist-packages (0.5.1)\n","Requirement already satisfied: JPype1>=0.5.7 in /usr/local/lib/python3.6/dist-packages (from konlpy) (0.7.0)\n","Requirement already satisfied: palettable in /usr/local/lib/python3.6/dist-packages (3.2.0)\n","Requirement already satisfied: colorbrewer in /usr/local/lib/python3.6/dist-packages (0.2.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from colorbrewer) (1.12.0)\n","*** Introductory Examples for the NLTK Book ***\n","Loading text1, ..., text9 and sent1, ..., sent9\n","Type the name of the text or sentence to view it.\n","Type: 'texts()' or 'sents()' to list the materials.\n","text1: Moby Dick by Herman Melville 1851\n","text2: Sense and Sensibility by Jane Austen 1811\n","text3: The Book of Genesis\n","text4: Inaugural Address Corpus\n","text5: Chat Corpus\n","text6: Monty Python and the Holy Grail\n","text7: Wall Street Journal\n","text8: Personals Corpus\n","text9: The Man Who Was Thursday by G . K . Chesterton 1908\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2_P8gvlFxuCu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":346},"outputId":"1680083d-e5f2-418a-a9c7-ddd11f1c7843","executionInfo":{"status":"ok","timestamp":1563784368087,"user_tz":-540,"elapsed":836,"user":{"displayName":"얍얍","photoUrl":"","userId":"00095991694523741423"}}},"source":["# KoNLPy\n","\n","# NLTK 는 영어 정보처리를 위한 패키지\n","# 반면, KoNLPy 는 한국어 정보처리를 위한 패키지임\n","\n","# 서울대학교 산업공학과에서 개발한 형태소 분석기 래퍼 패키지인데,\n","# 이미 개발된 한글 형태소 분석기를 파이썬에서 바로 사용할 수 있도록 도와줌\n","\n","# 현재까지 지원하는 형태소 분석기는 모두 5가지\n","# => Hannanum, Kkma, Komoran, mecab, Okt\n","\n","# konlpy-ko.readthedocs.io / konlpy.org\n","\n","# konlpy 설치전 필수 패키지가 JPype1 인데\n","# Visual C++ 14 버젼 필요!! => visualstudio\n","# ==> 하지만, 미리 컴파일된 패키지가 배포됨\n","\n","# www.lfd.uci.edu/~gohlke/pythonlibs 에서\n","# JPype1 검색후 패키지를 다운로드한 후 설치함\n","\n","# pip install JPype1-0.7.0-cp36-cp36m-win_amd64.whl => 콜랩아닌 프로그램에서 설치\n","# pip install konlpy \n","\n","##### ------------------ 본 한글주석 by 유형균 (이메일 문의 : quantgeni@gmail.com) ------------------ #####\n","\n","twitter = Okt()\n","# 트위터 형태소 사전을 사용하기 위해 초기화\n","\n","txt1 = '아버지가 방에 들어가신다'\n","txt2 = '나는 보리밥을 먹었다'\n","txt3 = '롯데마트가 판매하고 있는 흑마늘 양념치킨이 논란이 되고 있다'\n","\n","# 형태소 분석\n","# pos() 주요 품사 기반 추출\n","\n","print(twitter.pos(txt1))\n","print()\n","print(twitter.pos(txt2))\n","print()\n","print(twitter.pos(txt3))\n","print('-------------------------------------------------------------------------------------------------------')\n","\n","# 형태소 분석\n","# morphs() 상세 품사 기반 추출\n","print(twitter.morphs(txt1))\n","print()\n","print(twitter.morphs(txt2))\n","print()\n","print(twitter.morphs(txt3))\n","print('-------------------------------------------------------------------------------------------------------')\n","\n","# 형태소 분석\n","# nouns() 명사 기반 추출\n","# 출력물을 보면, preprocessing에 있어 상당히 편리하다는 것을 알 수 있다\n","print(twitter.nouns(txt1))\n","print()\n","print(twitter.nouns(txt2))\n","print()\n","print(twitter.nouns(txt3))"],"execution_count":9,"outputs":[{"output_type":"stream","text":["[('아버지', 'Noun'), ('가', 'Josa'), ('방', 'Noun'), ('에', 'Josa'), ('들어가신다', 'Verb')]\n","\n","[('나', 'Noun'), ('는', 'Josa'), ('보리밥', 'Noun'), ('을', 'Josa'), ('먹었다', 'Verb')]\n","\n","[('롯데', 'Noun'), ('마트', 'Noun'), ('가', 'Josa'), ('판매', 'Noun'), ('하고', 'Josa'), ('있는', 'Adjective'), ('흑마', 'Noun'), ('늘', 'Noun'), ('양념치킨', 'Noun'), ('이', 'Josa'), ('논란', 'Noun'), ('이', 'Josa'), ('되고', 'Verb'), ('있다', 'Adjective')]\n","-------------------------------------------------------------------------------------------------------\n","['아버지', '가', '방', '에', '들어가신다']\n","\n","['나', '는', '보리밥', '을', '먹었다']\n","\n","['롯데', '마트', '가', '판매', '하고', '있는', '흑마', '늘', '양념치킨', '이', '논란', '이', '되고', '있다']\n","-------------------------------------------------------------------------------------------------------\n","['아버지', '방']\n","\n","['나', '보리밥']\n","\n","['롯데', '마트', '판매', '흑마', '늘', '양념치킨', '논란']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AJG3hTEjzeFX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":308},"outputId":"f382c2f7-47fd-4984-943e-a46dc8269972","executionInfo":{"status":"ok","timestamp":1563787963550,"user_tz":-540,"elapsed":883,"user":{"displayName":"얍얍","photoUrl":"","userId":"00095991694523741423"}}},"source":["##### ------------------ 본 한글주석 by 유형균 (이메일 문의 : quantgeni@gmail.com) ------------------ #####\n","# 대한민국 헌법 말뭉치를 이용해서 워드클라우드로 시각화해 봄\n","# KoNLPy 내장 한국어 말뭉치\n","# 대한민국 헌법 말뭉치 : kolaw\n","# 국회법안 말뭉치 : kobill\n","\n","from konlpy.corpus import kolaw\n","from konlpy.corpus import kobill\n","\n","kolaw.fileids() # 말뭉치 이름 출력\n","kobill.fileids()\n","\n","doc1 = kolaw.open('constitution.txt').read()\n","print(doc1[:100]) # 100자까지 확인\n","print('-----------------------------------------------------------------------------------')\n","doc2 = kobill.open('1809895.txt').read()\n","print(doc2[:100]) # 100자까지 확인"],"execution_count":16,"outputs":[{"output_type":"stream","text":["대한민국헌법\n","\n","유구한 역사와 전통에 빛나는 우리 대한국민은 3·1운동으로 건립된 대한민국임시정부의 법통과 불의에 항거한 4·19민주이념을 계승하고, 조국의 민주개혁과 평화적 통일의\n","-----------------------------------------------------------------------------------\n","하도급거래 공정화에 관한 법률 일부개정법률안\n","\n","(유선호의원 대표발의 )\n","\n"," 의 안\n"," 번 호\n","\n","9895\n","\n","발의연월일 : 2010.  11.  15.\n","\n","발  의  자 : 유선호․강기갑\n"],"name":"stdout"}]}]}