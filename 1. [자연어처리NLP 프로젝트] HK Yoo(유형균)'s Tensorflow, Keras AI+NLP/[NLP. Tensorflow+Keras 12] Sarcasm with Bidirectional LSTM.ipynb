{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[NLP. Tensorflow+Keras 12] Sarcasm with Bidirectional LSTM.ipynb","version":"0.3.2","provenance":[{"file_id":"https://github.com/lmoroney/dlaicourse/blob/master/TensorFlow%20In%20Practice/Course%203%20-%20NLP/Course%203%20-%20Week%203%20-%20Lesson%202.ipynb","timestamp":1563779955923},{"file_id":"1OOzqEhzmkFQOtmEGwMhiLxB_D2osx4eM","timestamp":1556803950670}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"bXg0SwrTZ7Ja","colab_type":"text"},"source":["# 실제 예제를 통한 LSTM (양방향bi-directional) 성능 실험 프로젝트 by 유형균\n","## Multiple Layer LSTM"]},{"cell_type":"code","metadata":{"id":"jGwXGIXvFhXW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"e9d798e7-9661-4a41-9553-6c21d65a8d22"},"source":["##### ------------------ 본 한글주석 by 유형균 (이메일 문의 : quantgeni@gmail.com) ------------------ #####\n","# 전체적으로 살펴보자\n","# 핵심은, RNN의 bidirectional LSTM (양방향 LSTM)을 적용했다는 점이다. CNN 제외\n","# padding + embedding + LSTM\n","\n","\n","import json\n","import tensorflow as tf\n","\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","# json file \"sarcasm\"를 불러오자\n","!wget --no-check-certificate \\\n","    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sarcasm.json \\\n","    -O /tmp/sarcasm.json\n","\n","vocab_size = 1000\n","embedding_dim = 16\n","max_length = 120\n","trunc_type='post'\n","padding_type='post'\n","oov_tok = \"<OOV>\"\n","training_size = 20000\n","\n","\n","with open(\"/tmp/sarcasm.json\", 'r') as f:\n","    datastore = json.load(f)\n","\n","\n","sentences = []\n","labels = []\n","urls = []\n","\n","# headline과 비평 (label) 별도 리스트에 append\n","for item in datastore:\n","    sentences.append(item['headline'])\n","    labels.append(item['is_sarcastic'])\n","\n","# 수동으로 split\n","training_sentences = sentences[0:training_size]\n","testing_sentences = sentences[training_size:]\n","training_labels = labels[0:training_size]\n","testing_labels = labels[training_size:]\n","\n","tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n","tokenizer.fit_on_texts(training_sentences)\n","\n","word_index = tokenizer.word_index\n","\n","training_sequences = tokenizer.texts_to_sequences(training_sentences)\n","training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n","\n","testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n","testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n","\n","# 위에서 padding 적용했고, 당초 'post'로 설정했음에 유의 (default는 'pre')\n","\n","# Embedding 시켰다는 것에 유의!\n","model = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n","    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)), # LSTM을 단방향이 아닌, 양방향으로 설정!\n","    tf.keras.layers.Dense(24, activation='relu'),\n","    tf.keras.layers.Dense(1, activation='sigmoid')\n","])\n","\n","# optimizer 아담 + 손실함수 크로스엔트로피(단, binary로 출력)\n","model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n","model.summary()\n","\n","num_epochs = 50\n","\n","# fit시킨 결과는 history 변수에 담기\n","history = model.fit(training_padded, training_labels, epochs=num_epochs, validation_data=(testing_padded, testing_labels), verbose=1)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["--2019-07-22 11:04:15--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sarcasm.json\n","Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.124.128, 2607:f8b0:4001:c14::80\n","Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.124.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 5643545 (5.4M) [application/json]\n","Saving to: ‘/tmp/sarcasm.json’\n","\n","\r/tmp/sarcasm.json     0%[                    ]       0  --.-KB/s               \r/tmp/sarcasm.json   100%[===================>]   5.38M  --.-KB/s    in 0.05s   \n","\n","2019-07-22 11:04:15 (117 MB/s) - ‘/tmp/sarcasm.json’ saved [5643545/5643545]\n","\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING: Logging before flag parsing goes to stderr.\n","W0722 11:04:17.510818 140488951129984 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","W0722 11:04:17.514930 140488951129984 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","W0722 11:04:17.523665 140488951129984 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","W0722 11:04:17.525076 140488951129984 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","W0722 11:04:17.529583 140488951129984 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","W0722 11:04:17.987373 140488951129984 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n"],"name":"stderr"},{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding (Embedding)        (None, 120, 16)           16000     \n","_________________________________________________________________\n","bidirectional (Bidirectional (None, 64)                12544     \n","_________________________________________________________________\n","dense (Dense)                (None, 24)                1560      \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 1)                 25        \n","=================================================================\n","Total params: 30,129\n","Trainable params: 30,129\n","Non-trainable params: 0\n","_________________________________________________________________\n","Train on 20000 samples, validate on 6709 samples\n","Epoch 1/50\n","20000/20000 [==============================] - 304s 15ms/sample - loss: 0.4655 - acc: 0.7689 - val_loss: 0.3950 - val_acc: 0.8171\n","Epoch 2/50\n","20000/20000 [==============================] - 376s 19ms/sample - loss: 0.3571 - acc: 0.8378 - val_loss: 0.3772 - val_acc: 0.8289\n","Epoch 3/50\n","20000/20000 [==============================] - 294s 15ms/sample - loss: 0.3303 - acc: 0.8504 - val_loss: 0.3707 - val_acc: 0.8310\n","Epoch 4/50\n","20000/20000 [==============================] - 294s 15ms/sample - loss: 0.3152 - acc: 0.8604 - val_loss: 0.3741 - val_acc: 0.8298\n","Epoch 5/50\n","20000/20000 [==============================] - 374s 19ms/sample - loss: 0.3051 - acc: 0.8662 - val_loss: 0.3745 - val_acc: 0.8290\n","Epoch 6/50\n","19968/20000 [============================>.] - ETA: 0s - loss: 0.2992 - acc: 0.8665"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"g9DC6dmLF8DC","colab_type":"code","colab":{}},"source":["##### ------------------ 본 한글주석 by 유형균 (이메일 문의 : quantgeni@gmail.com) ------------------ #####\n","\n","import matplotlib.pyplot as plt\n","\n","\n","def plot_graphs(history, string):\n","  plt.plot(history.history[string])\n","  plt.plot(history.history['val_'+string])\n","  plt.xlabel(\"Epochs\")\n","  plt.ylabel(string)\n","  plt.legend([string, 'val_'+string])\n","  plt.show()\n","\n","plot_graphs(history, 'acc')\n","plot_graphs(history, 'loss')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7ZEZIUppGhdi","colab_type":"code","colab":{}},"source":["##### ------------------ 본 한글주석 by 유형균 (이메일 문의 : quantgeni@gmail.com) ------------------ #####\n","\n","model.save(\"test.h5\")"],"execution_count":0,"outputs":[]}]}